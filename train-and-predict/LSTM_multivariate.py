"""
This file is part of the conclusion work of my MBA studies on Data Science and
Analytics. Intention of this is to automate execution of an experiment using
LSTM technic (univariated and multivariated).

Note that part of the code used here was fairly inspired on other sources from
Dr. Sreenivas Bhattiprolu whom I am very thankful. Find original references at
  - https://github.com/bnsreenu/python_for_microscopists/blob/master/166a-Intro_to_time_series_Forecasting_using_LSTM.py
  - https://youtu.be/97bZKO6cJfg
  - https://github.com/bnsreenu/python_for_microscopists/blob/master/181_multivariate_timeseries_LSTM_GE.py
  - https://youtu.be/tepxdcepTbY

@author: Rafael Simionato
"""

import pickle
import math

import numpy as np
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, LSTM
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras.losses import MeanAbsoluteError
from tensorflow.keras.metrics import RootMeanSquaredError
from tensorflow.keras.optimizers import Adam

import pandas as pd
from matplotlib import pyplot as plt
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_squared_error

# Reloading df_filtered_cvm generated by cvm-dowloader code
input_file_path_cvm = '../cvm-downloader/data/filtered_dataset/df_filtered_cvm.pkl'
with open(input_file_path_cvm, 'rb') as f: df_cvm_filtered = pickle.load(f)

df_for_training = pd.DataFrame(df_cvm_filtered)

use_non_trivial_data = True
if use_non_trivial_data == True:
    print('Using trivial and non-trivial data for this analysis ...')

    # Reloading df_filtered_plublishedat_tfidf generated by youtube-scraper code
    input_file_path_youtube = '../youtube-scraper/data/filtered_dataset/df_filtered_plublishedat_tfidf.pkl'
    with open(input_file_path_youtube, 'rb') as f: df_filtered_plublishedat_tfidf = pickle.load(f)
    
    # Merging both trivial and non-trivial datasets
    df_for_training = df_for_training.join(df_filtered_plublishedat_tfidf
                                      .set_index('publishedAt_YMD'), on='DT_COMPTC_YMD')
else:
    print('Using only trivial data for this analysis ...')

cols_to_drop = [
        #'VL_QUOTA',
        'VL_TOTAL',
        'VL_PATRIM_LIQ',
        'CAPTC_DIA',
        'RESG_DIA',
        'NR_COTST',
        'DT_COMPTC_YMD'
        ]

df_for_training.drop(cols_to_drop, inplace = True, axis = 1)

df_for_training = df_for_training[df_for_training.columns].astype(float)

df_for_training.isnull().values.any()
df_for_training.isnull().sum().sum()
df_for_training = df_for_training.fillna(0)

# LSTM uses sigmoid and tanh that are sensitive to magnitude so values must to be normalized
scale_with_minmax = True
if scale_with_minmax == True:
    print('Scalling the dataset with MinMaxScaler')
    scaler = MinMaxScaler(feature_range=(0, 1))
    df_for_training_scaled = scaler.fit_transform(df_for_training)
else:
    print('Scalling the dataset with StandardScaler')
    scaler = StandardScaler()
    scaler = scaler.fit(df_for_training)
    df_for_training_scaled = scaler.transform(df_for_training)

# As required for LSTM networks, we must reshape input data into n_samples x timesteps x n_features.
def to_sequences(dataset, seq_size=1):
    x = []
    y = []
    for i in range(seq_size, len(dataset)):
        x.append(dataset[i - seq_size:i, 0:dataset.shape[1]])
        y.append(dataset[i:i + 1, 0])
    return np.array(x), np.array(y)

train_size = int(len(df_for_training_scaled) * 0.8)
test_size = len(df_for_training_scaled) - train_size
train, test = df_for_training_scaled[0:train_size,:], df_for_training_scaled[train_size:len(df_for_training_scaled),:]

# Number of time steps to look back
seq_size = 5

trainX = []
trainY = []
trainX, trainY = to_sequences(train, seq_size)
print("Shape of training set: {}".format(trainX.shape))

testX = []
testY = []
testX, testY = to_sequences(test, seq_size)
print("Shape of test set: {}".format(testX.shape))

print('Single LSTM with hidden Dense...')

model = Sequential()
# model.add(LSTM(64, activation='relu', input_shape=(trainX.shape[1], trainX.shape[2]), return_sequences=True))
model.add(LSTM(64, activation='relu', input_shape=(trainX.shape[1], trainX.shape[2])))
model.add(Dense(32))
model.add(Dense(1))

cp = EarlyStopping(monitor='val_loss', min_delta=1e-5, patience=40, verbose=1, mode='auto', restore_best_weights=True)

model.compile(optimizer=Adam(learning_rate=0.00001),
              loss=MeanAbsoluteError(),              
              metrics=[RootMeanSquaredError()])

model.summary()

print('Let us train it !!!')

history = model.fit(trainX, trainY,
                    epochs=1000, batch_size=32,
                    validation_split=0.1,
                    callbacks=[cp],
                    verbose=1)

val_loss_min = min(history.history['val_loss'])
best_epoch = history.history['val_loss'].index(val_loss_min) + 1

plt.figure(num = 0, figsize=(9, 5))
fig_title = 'Training and validation metrics upon model fitting\n' + \
            'Best epoch { %d }\n' % (best_epoch) + \
            'Loss { train: %.6f | validation: %.6f }\n' % (history.history['loss'][best_epoch-1], history.history['val_loss'][best_epoch-1]) + \
            'RMSE { train: %.6f | validation: %.6f }' % (history.history['root_mean_squared_error'][best_epoch-1], history.history['val_root_mean_squared_error'][best_epoch-1])
plt.title(fig_title)
plt.plot(history.history['loss'], label='Training loss')
plt.plot(history.history['val_loss'], label='Validation loss')
plt.plot(history.history['root_mean_squared_error'], label='Training root_mean_squared_error')
plt.plot(history.history['val_root_mean_squared_error'], label='Validation root_mean_squared_error')
plt.xlabel('epoch')
plt.legend()

# Making predictions
trainPredictedScaled = model.predict(trainX)
testPredictedScaled = model.predict(testX)

# Perform inverse transformation to rescale back to original range
# Since we used 'x' variables for transform, the inverse expects same dimensions
# Therefore, let us copy our values 'x' times and discard them after inverse transform
def apply_inverse_transform(array_values):
    array_values_copies = np.repeat(array_values, df_for_training_scaled.shape[1], axis=-1)
    array_values_unscaled = scaler.inverse_transform(array_values_copies)[:,0]
    return array_values_unscaled

# Invert predictions back to prescaled values to compare with original input values
# As we used minmaxscaler, we can use scaler.inverse_transform to invert the transformation.
trainPredictedUnscaled = apply_inverse_transform(trainPredictedScaled)
trainYUnscaled = apply_inverse_transform(trainY)
testPredictedUnscaled = apply_inverse_transform(testPredictedScaled)
testYUnscaled = apply_inverse_transform(testY)

# Calculating root mean squared error
trainScore = math.sqrt(mean_squared_error(trainYUnscaled, trainPredictedUnscaled))
print('Train Score: %.6f RMSE' % (trainScore))

testScore = math.sqrt(mean_squared_error(testYUnscaled, testPredictedUnscaled))
print('Test Score: %.6f RMSE' % (testScore))

# Shifting predictions so that they align on the x-axis with the original dataset. 
trainPredictPlot = np.empty_like(df_for_training.iloc[:, 0])
trainPredictPlot[:] = np.nan
trainPredictPlot[seq_size:len(trainPredictedUnscaled)+seq_size] = trainPredictedUnscaled

testPredictPlot = np.empty_like(df_for_training.iloc[:, 0])
testPredictPlot[:] = np.nan
testPredictPlot[len(trainPredictedUnscaled)+(seq_size*2):len(df_for_training)] = testPredictedUnscaled

df_plot = pd.DataFrame(data = {'Original data': df_for_training.iloc[:, 0].values,
                              'Prediction on train': trainPredictPlot,
                              'Prediction on test': testPredictPlot},
                       index = df_for_training.index)

plt.figure(num = 1, figsize=(9, 5))
fig_title = 'Predictions vs Original data\n' + \
            'RMSE { train: %.6f | test: %.6f }\n' % (trainScore, testScore) + \
            'Input dataset with %d feature' % (df_for_training.shape[1])
if df_for_training.shape[1] > 1: fig_title = fig_title + 's'
plt.title(fig_title)
ax = df_plot.plot(ax=plt.gca())
lines, labels = ax.get_legend_handles_labels()
ax.legend(lines[:3], labels[:3], loc='best')
ax.set(xlabel=None)